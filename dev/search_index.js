var documenterSearchIndex = {"docs":
[{"location":"getting_started/#Getting-Started-with-DecisionTreeAndRandomForest.jl","page":"Getting Started","title":"Getting Started with DecisionTreeAndRandomForest.jl","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"This guide will introduce you to the basics of using classification trees and random forests with the DecisionTreeAndRandomForest.jl package.","category":"page"},{"location":"getting_started/#What-is-a-Decision-Tree?","page":"Getting Started","title":"What is a Decision Tree?","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"A decision tree is a machine learning model used for both classification and regression tasks. It uses a tree-like structure where internal nodes represent features, branches represent decision rules, and each leaf node represents an outcome. Decision trees are easy to interpret and visualize, making them popular for many applications.","category":"page"},{"location":"getting_started/#What-is-a-Random-Forest?","page":"Getting Started","title":"What is a Random Forest?","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"A random forest is an ensemble learning method that constructs multiple decision trees during training and outputs the mode of the classes (classification) or mean prediction (regression) of the individual trees. This approach improves accuracy and reduces overfitting.","category":"page"},{"location":"getting_started/#Overview-of-Features","page":"Getting Started","title":"Overview of Features","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Classification Trees: Build trees for classifying data.\nRegression Trees: Construct trees for predicting continuous values.\nRandom Forests: Ensemble method that combines multiple decision trees to improve accuracy and robustness.\nCustom Splitting Criteria: Support for various splitting criteria such as Gini Impurity, Information Gain, and Variance Reduction.","category":"page"},{"location":"getting_started/#Basic-Example-of-a-Classification-Tree","page":"Getting Started","title":"Basic Example of a Classification Tree","text":"","category":"section"},{"location":"getting_started/#Step-1:-Import-the-Module","page":"Getting Started","title":"Step 1: Import the Module","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"First, import the DecisionTreeAndRandomForest module:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using DecisionTreeAndRandomForest","category":"page"},{"location":"getting_started/#Step-2:-Prepare-Training-Data","page":"Getting Started","title":"Step 2: Prepare Training Data","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Prepare some training data and their respective labels:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"data = [\"dog\" 37.0; \"dog\" 38.4; \"dog\" 40.2; \"dog\" 38.9; \"human\" 36.2; \"human\" 37.4; \"human\" 38.8; \"human\" 36.2]\nlabels = [\"healthy\", \"healthy\", \"sick\", \"healthy\", \"healthy\", \"sick\", \"sick\", \"healthy\"]\nnothing # hide","category":"page"},{"location":"getting_started/#Step-3:-Initialize-a-Tree","page":"Getting Started","title":"Step 3: Initialize a Tree","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Initialize a classification tree:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"tree = DecisionTree(split_gini)\nnothing # hide","category":"page"},{"location":"getting_started/#Step-4:-Build-the-Tree","page":"Getting Started","title":"Step 4: Build the Tree","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Build the tree using the fit! function:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"fit!(tree, data, labels)\nnothing # hide","category":"page"},{"location":"getting_started/#Step-5:-Print-the-Tree","page":"Getting Started","title":"Step 5: Print the Tree","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To inspect the tree structure, simply print the tree:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"print(tree)","category":"page"},{"location":"getting_started/#Step-6:-Classify-Test-Samples","page":"Getting Started","title":"Step 6: Classify Test Samples","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Create some test samples for classification:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"test_data = [\"dog\" 38.0; \"human\" 38.0]\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We expect the output to be healthy for the first sample and sick for the second one.","category":"page"},{"location":"getting_started/#Step-7:-Predict-Labels","page":"Getting Started","title":"Step 7: Predict Labels","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Retrieve the labels assigned to the test samples using the predict function:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"prediction = predict(tree, test_data)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"By following these steps, you can create and use a basic classification tree. This example illustrates how decision trees can be applied to simple datasets for classification tasks.","category":"page"},{"location":"getting_started/#Basic-Example-of-a-Random-Forest","page":"Getting Started","title":"Basic Example of a Random Forest","text":"","category":"section"},{"location":"getting_started/#Step-1:-Prepare-Training-Data","page":"Getting Started","title":"Step 1: Prepare Training Data","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Use the same training data and labels as before:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"data = [\"dog\" 37.0; \"dog\" 38.4; \"dog\" 40.2; \"dog\" 38.9; \"human\" 36.2; \"human\" 37.4; \"human\" 38.8; \"human\" 36.2]\nlabels = [\"healthy\", \"healthy\", \"sick\", \"healthy\", \"healthy\", \"sick\", \"sick\", \"healthy\"]\nnothing # hide","category":"page"},{"location":"getting_started/#Step-2:-Initialize-a-Random-Forest","page":"Getting Started","title":"Step 2: Initialize a Random Forest","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Initialize a random forest with the specified parameters:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"forest = RandomForest(split_gini)\nnothing # hide","category":"page"},{"location":"getting_started/#Step-3:-Build-the-Random-Forest","page":"Getting Started","title":"Step 3: Build the Random Forest","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Build the random forest using the fit! function:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"fit!(forest, data, labels)\nnothing # hide","category":"page"},{"location":"getting_started/#Step-4:-Predict-Labels","page":"Getting Started","title":"Step 4: Predict Labels","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Create some test samples for classification:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"test_data = [\"dog\" 38.0; \"human\" 38.0]\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Retrieve the labels assigned to the test samples using the predict function:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"forest_predictions = predict(forest, test_data)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"By following these steps, you can create and use a basic random forest. This example illustrates how random forests can be applied to simple datasets for classification tasks.","category":"page"},{"location":"getting_started/#Adding-More-Splitting-Criteria","page":"Getting Started","title":"Adding More Splitting Criteria","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To add more splitting criteria, define a new function that computes the desired criterion. For example, to implement a Chi-Squared Split:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"function chi_squared_split(data, labels, num_features)\n    # Implementation of Chi-Squared split criterion\nend\nnothing # hide","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Then use this new function when creating the tree:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"tree = DecisionTree(-1, 1, -1, chi_squared_split)\nnothing # hide","category":"page"},{"location":"splitting_criterion/#Splitting-Criterion","page":"Splitting Criterion","title":"Splitting Criterion","text":"","category":"section"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"This package offers multiple options of splitting criterion for evaluating the quality of a split and therefore constructing a decision tree. In the following a brief overview of the available criterion and their use cases is provided.","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"You can retrieve a list of the available splitting criterions with the function get_split_criterions, like so:","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"using DecisionTreeAndRandomForest # hide\nprintln(get_split_criterions())\nprintln(get_split_criterions(\"classification\"))\nprintln(get_split_criterions(\"regression\"))","category":"page"},{"location":"splitting_criterion/#Gini-Impurity","page":"Splitting Criterion","title":"Gini Impurity","text":"","category":"section"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"Gini Impurity measures the likelihood of an incorrect classification of a randomly chosen element if it is labeled according to the distribution of labels in the dataset.  Gini Impurity is primarily used in classification problems. A lower Gini impurity indicates a purer node with a higher confidence in predicting the class label. The goal is to minimize the Gini Impurity at each split, thereby creating nodes that are as pure as possible.","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"Gini(S) = 1 - sum_i=1^C (p_i)^2","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"where:","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"S is the set of data points in the current node.\n\nC is the number of classes.\n\np_i is the proportion of data points belonging to class i in S.\n","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"Steps for Calculation:","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"Calculate Gini coefficients for each child node.\nCompute the impurity for each split using a weighted Gini score.\nChoose the split with the lowest Gini impurity.","category":"page"},{"location":"splitting_criterion/#Information-Gain","page":"Splitting Criterion","title":"Information Gain","text":"","category":"section"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"Information Gain calculated as the difference in entropy before and after splitting a dataset on an attribute. Entropy measures the uncertainty or impurity in the data. The goal is to reduce entropy and maximize information gain, leading to a more informative split. Information Gain is used in classification problems to choose the attribute that provides the highest information gain, resulting in the most informative split.","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"The information gain for a dataset ùëÜ after a split on attribute ùê¥ is given by:","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"Gain(S A) = Entropy(S) - sum_v in textValues(A) fracS_vS Entropy(S_v)\n","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"where:","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"ùê¥ is a feature of the dataset.\n\nv is a specific value of the feature ùê¥.\n  \n","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"Entropy is calculated as:","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"Entropy(S) = -sum_i=1^C p_i log_2(p_i)","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"where:","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"S is the set of data points in the current node.\n\nC is the number of classes.\n\np_i is the proportion of data points belonging to class i in S.\n","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"\nSteps for Calculation:","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"Calculate the entropy of the original dataset ùëÜ.\nFor each split on attribute ùê¥ calculate the entropy of each child node ùëÜ_ùë£ and calculate the weighted entropy after the split.\nCompute the Information Gain by subtracting the weighted entropy from the original entropy.\nChoose the split with the highest information gain.","category":"page"},{"location":"splitting_criterion/#Variance-Reduction","page":"Splitting Criterion","title":"Variance Reduction","text":"","category":"section"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"Variance Reduction measures the reduction in variance of the target variable achieved by splitting a node. Higher variance reduction indicates a more informative split. Variance reduction is particularly useful for regression problems where the goal is to predict a continuous target variable.","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"textVR(S) = sigma^2(S) - sum_i=1^n fracS_iS sigma^2(S_i)","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"where:","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"sigma^2(S): Variance of the parent node S.\nS: Set of data points in the current node.\nS_i: Subsets of S after the split.\nn: Number of subsets after the split.","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"Steps for Calculation:","category":"page"},{"location":"splitting_criterion/","page":"Splitting Criterion","title":"Splitting Criterion","text":"Calculate the variance of the parent node S.\nFor each child node S_i, calculate its variance.\nCompute the weighted sum of the variances of the child nodes S_i.\nSubtract the weighted sum from the variance of the parent node S to get the variance reduction.\nChoose the split with the highest variance reduction.","category":"page"},{"location":"dataset_examples/#Decision-Trees-and-Random-Forests-on-Real-World-Datasets","page":"Dataset Examples","title":"Decision Trees and Random Forests on Real-World Datasets","text":"","category":"section"},{"location":"dataset_examples/#Classification-Tree-and-Random-Forest-Classifying-Iris-Types","page":"Dataset Examples","title":"Classification Tree and Random Forest - Classifying Iris Types","text":"","category":"section"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"In this tutorial, we will demonstrate how to use the package to create a classification tree and a random forest, and apply them to classify the Iris dataset.","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"First, ensure to import all the necessary packages:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"using DataFrames\nusing MLJ: load_iris, unpack, partition\nusing DecisionTreeAndRandomForest","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"Second, we load the Iris dataset and prepare the data by splitting it into training and test sets:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"data = load_iris()\niris = DataFrame(data)\ny, X = unpack(iris, ==(:target); rng=123)\ntrain, test = partition(eachindex(y), 0.7)\ntrain_labels = Vector{String}(y[train])\ntest_labels = Vector{String}(y[test])\ntrain_data = Matrix(X[train, :])\ntest_data = Matrix(X[test, :])\nnothing # hide","category":"page"},{"location":"dataset_examples/#Decision-Tree","page":"Dataset Examples","title":"Decision Tree","text":"","category":"section"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"Next, we create the decision tree and fit it to the training data:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"# Initialize the classification tree with hyperparameters:\n# - max_depth: Maximum depth of the tree (-1 means no limit).\n# - min_samples_split: Minimum number of samples required to split an internal node.\n# - num_features: Number of features to consider when looking for the best split (-1 means all features).\n# - split_criterion: Function to measure the quality of a split.\ntree = DecisionTree(-1, 1, -1, split_gini)\nfit!(tree, train_data, train_labels)\nnothing # hide","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"Now we can use the Decision Tree to make predictions for unseen data:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"predictions = predict(tree, test_data)\nprintln(\"Decision Tree - Correct label: \", test_labels[1])\nprintln(\"Decision Tree - Predicted label: \", predictions[1])\nnothing # hide","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"We can also test the accuracy of our Classification Tree:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"accuracy = sum(predictions .== test_labels) / length(test_labels)","category":"page"},{"location":"dataset_examples/#Random-Forest","page":"Dataset Examples","title":"Random Forest","text":"","category":"section"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"Next, we create the random forest and fit it to the training data:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"# Initialize the random forest with hyperparameters:\n# - max_depth: Maximum depth of the trees (-1 means no limit).\n# - min_samples_split: Minimum number of samples required to split an internal node.\n# - split_criterion: Function to measure the quality of a split.\n# - number_of_trees: Number of trees in the forest.\n# - subsample_percentage: Percentage of samples to use for each tree (0.8 means 80% of the data).\n# - num_features: Number of features to consider when looking for the best split (-1 means all features).\nforest = RandomForest(7, 3, split_gini, 15, 0.7, -1)  # Using 7 for max_depth, 3 for min_samples_split, 15 for number_of_trees, and 0.7 for subsample_percentage\nfit!(forest, train_data, train_labels)","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"Now we can use the Random Forest to make predictions for unseen data:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"forest_predictions = predict(forest, test_data)\nprintln(\"Random Forest - Correct label: \", test_labels[1])\nprintln(\"Random Forest - Predicted label: \", forest_predictions[1])\nnothing # hide","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"We can also test the accuracy of our Random Forest:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"forest_accuracy = sum(forest_predictions .== test_labels) / length(test_labels)\nnothing # hide","category":"page"},{"location":"dataset_examples/#Regression-Tree-and-Random-Forest-Predicting-Housing-Prices","page":"Dataset Examples","title":"Regression Tree and Random Forest - Predicting Housing Prices","text":"","category":"section"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"We will now demonstrate how to use the package to create a regression tree and a random forest, and apply them to the Boston Housing dataset. The dataset contains information collected by the U.S. Census Service concerning housing in the area of Boston, Massachusetts. It is often used to predict the median value of owner-occupied homes (in $1000s).","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"First, ensure to import all the necessary packages:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"using MLJ: unpack, partition\nusing RDatasets: dataset\nusing DataFrames\nusing DecisionTreeAndRandomForest\nusing Statistics: mean","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"Second, we load the Boston Housing dataset and prepare the data by splitting it into training and test sets:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"boston = dataset(\"MASS\", \"Boston\")\ndata = DataFrame(boston)\nX = data[:, 1:end-1]\ny = data[:, end]\ntrain_indices, test_indices = partition(eachindex(y), 0.95, rng=123)\ntrain_labels = Vector{Float64}(y[train_indices])\ntest_labels = Vector{Float64}(y[test_indices])\ntrain_data = Matrix(X[train_indices, :])\ntest_data = Matrix(X[test_indices, :])\nnothing # hide","category":"page"},{"location":"dataset_examples/#Decision-Tree-2","page":"Dataset Examples","title":"Decision Tree","text":"","category":"section"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"Next, we create the decsion tree and fit it to the training data:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"tree = DecisionTree(-1, 1, -1, split_variance)\nfit!(tree, train_data, train_labels)\nnothing # hide","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"Now we can use the Regression Tree to make predictions for unseen data:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"predictions = predict(tree, test_data)\nnothing # hide","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"We can also assess the quality of our Regression Tree:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"mse = mean((predictions .- test_labels).^2)\nprintln(\"Decision Tree - Mean Squared Error: \", mse)\nss_res = sum((test_labels .- predictions).^2)\nss_tot = sum((test_labels .- mean(test_labels)).^2)\nr2_score = 1 - (ss_res / ss_tot)\nprintln(\"Decision Tree - R¬≤ Score: \", r2_score)\nnothing # hide","category":"page"},{"location":"dataset_examples/#Random-Forest-2","page":"Dataset Examples","title":"Random Forest","text":"","category":"section"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"Next, we create the random forest and fit it to the training data:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"forest = RandomForest(-1, 1, split_variance, 10, 0.8, -1)\nfit!(forest, train_data, train_labels)\nnothing # hide","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"Now we can use the Random Forest to make predictions for unseen data:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"forest_predictions = predict(forest, test_data)\nnothing # hide","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"We can also assess the quality of our Random Forest:","category":"page"},{"location":"dataset_examples/","page":"Dataset Examples","title":"Dataset Examples","text":"forest_mse = mean((forest_predictions .- test_labels).^2)\nprintln(\"Random Forest - Mean Squared Error: \", forest_mse)\nforest_ss_res = sum((test_labels .- forest_predictions).^2)\nforest_ss_tot = sum((test_labels .- mean(test_labels)).^2)\nforest_r2_score = 1 - (forest_ss_res / forest_ss_tot)\nprintln(\"Random Forest - R¬≤ Score: \", forest_r2_score)\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = DecisionTreeAndRandomForest","category":"page"},{"location":"#DecisionTreeAndRandomForest","page":"Home","title":"DecisionTreeAndRandomForest","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for DecisionTreeAndRandomForest.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [DecisionTreeAndRandomForest]","category":"page"},{"location":"#DecisionTreeAndRandomForest.DecisionTree","page":"Home","title":"DecisionTreeAndRandomForest.DecisionTree","text":"Represents a DecisionTree.\n\nFields\n\nmax_depth::Int64: Controls the maximum depth of the tree. If -1, the DecisionTree is of unlimited depth.\nmin_samples_split::Int64: Controls the minimum number of samples required to split a node.\nnum_features::Int64: Controls the number of features to consider for each split. If -1, all features are used.\nsplit_criterion::Function: Contains the split criterion function.\nroot::Union{Missing, DecisionTreeAndRandomForest.Node, DecisionTreeAndRandomForest.Leaf}: Contains the root node of the DecisionTree.\n\n\n\n\n\n","category":"type"},{"location":"#DecisionTreeAndRandomForest.Leaf","page":"Home","title":"DecisionTreeAndRandomForest.Leaf","text":"Represents a Leaf in the DecisionTree structure.\n\nFields\n\nvalues::AbstractVector\n\n\n\n\n\n","category":"type"},{"location":"#DecisionTreeAndRandomForest.Node","page":"Home","title":"DecisionTreeAndRandomForest.Node","text":"Represents a Node in the DecisionTree structure.\n\nFields\n\nleft::Union{DecisionTreeAndRandomForest.Node, DecisionTreeAndRandomForest.Leaf}: Points to the left child.\nright::Union{DecisionTreeAndRandomForest.Node, DecisionTreeAndRandomForest.Leaf}: Points to the right child.\nfeature_index::Int64: Stores the index of the selected feature.\nsplit_value::Any: Stores the value on that the data is split.\n\n\n\n\n\n","category":"type"},{"location":"#DecisionTreeAndRandomForest.RandomForest","page":"Home","title":"DecisionTreeAndRandomForest.RandomForest","text":"Represents a RandomForest.\n\nFields\n\ntrees::Vector{DecisionTree}: Contains the vector of DecisionTree structures.\nmax_depth::Int64: Contains the maximum depth of the tree. If -1, the DecisionTree is of unlimited depth.\nmin_samples_split::Int64: Contains the minimum number of samples required to split a node.\nsplit_criterion::Function: Contains the split criterion function.\nnumber_of_trees::Int64: Contains the number of trees in the RandomForest structure.\nsubsample_percentage::Float64: Contains the percentage of the dataset to use for training each tree.\nnum_features::Int64: Contains the number of features to use when finding the best split. If -1, all the features are used.\n\n\n\n\n\n","category":"type"},{"location":"#Base.show-Tuple{IO, DecisionTree}","page":"Home","title":"Base.show","text":"show(io, tree)\n\n\nThis function recursively prints the structure of the DecisionTree, providing information about each node and leaf. It's primarily used for debugging and visualizing the tree's structure.\n\nArguments\n\nio::IO: The IO context to print the tree structure.\ntree::DecisionTree: The DecisionTree to print.\n\nReturns\n\nNothing: This function prints the structure of the DecisionTree.\n\n\n\n\n\n","category":"method"},{"location":"#Base.show-Tuple{IO, RandomForest}","page":"Home","title":"Base.show","text":"show(io, forest)\n\n\nThis function recursively prints the structure of the ForestTree. It's primarily used for debugging and visualizing the Forest structure.\n\nArguments\n\nio::IO: The IO context to print the Forest structure.\nforest::RandomForest: The RandomForest to be printed.\n\nReturns\n\nNothing: This function prints the structure of the RandomForest.\n\n\n\n\n\n","category":"method"},{"location":"#DecisionTreeAndRandomForest.best_split","page":"Home","title":"DecisionTreeAndRandomForest.best_split","text":"best_split(X, y)\nbest_split(X, y, num_features_to_use)\n\n\nFind the best split for the dataset X and labels y based on Information Gain. Returns the best feature and threshold for the split.\n\nArguments\n\nX::AbstractMatrix: A matrix of features.\ny::AbstractVector: A vector of labels.\nnum_features_to_use::Int=-1: The number of features to consider for each split. If -1, all features are used.\n\nReturns\n\nbest_feature::Int: The index of the best feature to split on.\nbest_threshold::Real: The threshold value for the best split.\n\n\n\n\n\n","category":"function"},{"location":"#DecisionTreeAndRandomForest.build_tree","page":"Home","title":"DecisionTreeAndRandomForest.build_tree","text":"build_tree(\n    data,\n    labels,\n    max_depth,\n    min_samples_split,\n    num_features,\n    split_criterion\n)\nbuild_tree(\n    data,\n    labels,\n    max_depth,\n    min_samples_split,\n    num_features,\n    split_criterion,\n    depth\n)\n\n\nThis function recursively builds a DecisionTree by iteratively splitting the data based on the provided split_criterion. The process continues until either the maximum depth is reached, the number of samples in a node falls below min_samples_split or all labels in a node are the same.\n\nArguments\n\ndata::AbstractMatrix: The training data.\nlabels::AbstractVector: The labels for the training data.\nmax_depth::Int: The maximum depth of the tree.\nmin_samples_split::Int: The minimum number of samples required to split a node.\nsplit_criterion::Function: The function used to determine the best split at each node.\ndepth::Int=0: The current depth of the tree (used recursively).\n\nReturns\n\nUnion{Node, Leaf}: The return value can be one of two types, depending on the state of the tree at each point of recursion.\n\n\n\n\n\n","category":"function"},{"location":"#DecisionTreeAndRandomForest.entropy-Tuple{AbstractVector}","page":"Home","title":"DecisionTreeAndRandomForest.entropy","text":"entropy(y)\n\n\nCalculates the entropy of a vector of labels y.\n\nArguments\n\ny::AbstractVector: A vector of labels.\n\nReturns\n\nFloat64: The entropy of the vector.\n\n\n\n\n\n","category":"method"},{"location":"#DecisionTreeAndRandomForest.find_best_split","page":"Home","title":"DecisionTreeAndRandomForest.find_best_split","text":"find_best_split(data, labels)\nfind_best_split(data, labels, num_features_to_use)\n\n\nFinds the best split point for a decision tree node. For now it uses the Gini impurity as splitting criterion, but should later be extended to support other criteria.\n\nArguments\n\ndata::AbstractMatrix: A matrix of features, where each row is a data point and each column is a feature.\nlabels::AbstractVector: A vector of labels corresponding to the data points.\nnum_features_to_use::Int: The number of features to consider when looking for the best split. If -1, all features are considered.\n\nReturns\n\nTuple{Int, T}: A tuple containing the index of the best feature and the best split value.\n\n\n\n\n\n","category":"function"},{"location":"#DecisionTreeAndRandomForest.find_best_split_vr","page":"Home","title":"DecisionTreeAndRandomForest.find_best_split_vr","text":"find_best_split_vr(data, labels)\nfind_best_split_vr(data, labels, num_features_to_use)\n\n\nFinds the best split point for a decision tree node using variance reduction.\n\nArguments\n\ndata::AbstractMatrix: A matrix of features, where each row is a data point and each column is a feature.\nlabels::AbstractVector: A vector of labels corresponding to the data points.\nnum_features_to_use::Int=-1: The number of features to consider for each split. If -1, all features are used.\n\nReturns\n\nTuple{Int, Any}: A tuple containing the index of the best feature and the best split value.\n\n\n\n\n\n","category":"function"},{"location":"#DecisionTreeAndRandomForest.fit!-Tuple{DecisionTree, AbstractMatrix, AbstractVector}","page":"Home","title":"DecisionTreeAndRandomForest.fit!","text":"fit!(tree, data, labels)\n\n\nThis function builds the tree structure of the DecisionTree by calling the build_tree function. \n\nArguments\n\ntree::DecisionTree: The DecisionTree to fit.\ndata::AbstractMatrix: The training data.\nlabels::AbstractVector: The labels for the training data.\n\nReturns\n\nNothing: This function modifies the tree in-place.\n\n\n\n\n\n","category":"method"},{"location":"#DecisionTreeAndRandomForest.fit!-Tuple{RandomForest, AbstractMatrix, AbstractVector}","page":"Home","title":"DecisionTreeAndRandomForest.fit!","text":"fit!(forest, data, labels)\n\n\nThis function trains each individual tree in the RandomForest by calling the fit function on each ClassificationTree within the forest.trees vector. The num_features parameter from the RandomForest object is used to control the number of features considered for each split during training.\n\nArguments\n\nforest::RandomForest: The RandomForest to be trained.\n\nReturns\n\nNothing: This function modifies the forest in-place.\n\n\n\n\n\n","category":"method"},{"location":"#DecisionTreeAndRandomForest.get_split_criterions","page":"Home","title":"DecisionTreeAndRandomForest.get_split_criterions","text":"get_split_criterions()\nget_split_criterions(task)\n\n\nRetrieves the implemented split criterions that can be used.\n\nArguments\n\ntask::String: A String that indicates for which task the splitting criterions should be retrieved. Can be \"classification\" or \"regression\".   Defaults to returning all available criterions\n\nReturns\n\nTuple{Function}: A tuple containing the implemented functions.\n\n\n\n\n\n","category":"function"},{"location":"#DecisionTreeAndRandomForest.gini_impurity-Tuple{AbstractVector}","page":"Home","title":"DecisionTreeAndRandomForest.gini_impurity","text":"gini_impurity(labels)\n\n\nThis function calculates the Gini impurity of a set of labels, which measures the homogeneity of the labels within a node. A lower Gini impurity indicates a more homogeneous set of labels.\n\nArguments\n\nlabels::AbstractVector: A vector of labels.\n\nReturns\n\nFloat64: The Gini impurity of the labels.\n\n\n\n\n\n","category":"method"},{"location":"#DecisionTreeAndRandomForest.information_gain-Union{Tuple{T}, Tuple{T, T, T}} where T<:(AbstractVector)","page":"Home","title":"DecisionTreeAndRandomForest.information_gain","text":"information_gain(y, y_left, y_right)\n\n\nCalculate the Information Gain of a split.\n\nArguments\n\ny::AbstractVector{T}: The original labels vector.\ny_left::AbstractVector{T}: The labels vector for the left split.\ny_right::AbstractVector{T}: The labels vector for the right split.\n\nReturns\n\nFloat64: The Information Gain of the split.\n\n\n\n\n\n","category":"method"},{"location":"#DecisionTreeAndRandomForest.predict-Tuple{DecisionTree, AbstractMatrix}","page":"Home","title":"DecisionTreeAndRandomForest.predict","text":"predict(tree, data)\n\n\nThis function traverses the tree structure of the DecisionTree for each datapoint in data. It follows the decision rules based on the split criteria and feature values. If the leaf node contains numerical values, its treated as a regreesion problem and the prediction is the average of those values. If a leaf node contains numerical values, it is treated as a regression problem, and the prediction is the average of those values. If the leaf node contains categorical labels, it is treated as a classification problem, and the prediction is the most frequent label (mode) among the labels in the leaf node.\n\nArguments\n\ntree::DecisionTree: The trained DecisionTree.\ndata::AbstractMatrix: The datapoints to predict.\n\nReturns\n\nVector: A vector of predictions for each datapoint in data.\n\n\n\n\n\n","category":"method"},{"location":"#DecisionTreeAndRandomForest.predict-Tuple{RandomForest, AbstractMatrix}","page":"Home","title":"DecisionTreeAndRandomForest.predict","text":"predict(forest, data)\n\n\nThis function predicts the labels for each datapoint in the input dataset by using the trained RandomForest.  Currently, it makes predictions using each individual tree in the forest and then combines the predictions using the most frequent label for each datapoint (Classification Task).\n\nArguments\n\nforest::RandomForest: The trained RandomForest.\ndata::AbstractMatrix: The dataset for which to make predictions.\n\nReturns\n\nAbstractVector: A vector of predictions for each datapoint in data.\n\n\n\n\n\n","category":"method"},{"location":"#DecisionTreeAndRandomForest.split_dataset-Tuple{AbstractMatrix, AbstractVector, Int64, Real}","page":"Home","title":"DecisionTreeAndRandomForest.split_dataset","text":"split_dataset(X, y, feature, threshold)\n\n\nSplit the dataset X and labels y based on a feature and a threshold. Returns the left and right splits for both X and y.\n\nArguments\n\nX::AbstractMatrix: A matrix of features.\ny::AbstractVector: A vector of labels.\nfeature::Int: The index of the feature to split on.\nthreshold::Real: The threshold value to split the feature.\n\nReturns\n\nX_left::AbstractMatrix, y_left::AbstractVector: The left split of the dataset and labels.\nX_right::AbstractMatrix, y_right::AbstractVector: The right split of the dataset and labels.\n\n\n\n\n\n","category":"method"},{"location":"#DecisionTreeAndRandomForest.split_gini","page":"Home","title":"DecisionTreeAndRandomForest.split_gini","text":"split_gini(data, labels)\nsplit_gini(data, labels, num_features)\n\n\nThis function is a wrapper for find_best_split to be used as the split criterion in the build_tree function.\n\nArguments\n\ndata::AbstractMatrix: A matrix of features, where each row is a data point and each column is a feature.\nlabels::AbstractVector: A vector of labels corresponding to the data points.\nnum_features::Int: The number of features to consider when looking for the best split. If -1, all features are considered.\n\nReturns\n\nTuple{Int, T}: A tuple containing the index of the best feature and the best split value.\n\n\n\n\n\n","category":"function"},{"location":"#DecisionTreeAndRandomForest.split_ig","page":"Home","title":"DecisionTreeAndRandomForest.split_ig","text":"split_ig(data, labels)\nsplit_ig(data, labels, num_features)\n\n\nThis function is a wrapper for best_split to be used as the split criterion in the build_tree function.\n\nArguments\n\ndata::AbstractMatrix: A matrix of features, where each row is a data point and each column is a feature.\nlabels::AbstractVector: A vector of labels corresponding to the data points.\nnum_features::Int: The number of features to consider for each split.\n\nReturns\n\nTuple{Int, Real}: A tuple containing the index of the best feature and the best split value.\n\n\n\n\n\n","category":"function"},{"location":"#DecisionTreeAndRandomForest.split_node-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector, Int64, T}} where T","page":"Home","title":"DecisionTreeAndRandomForest.split_node","text":"split_node(data, labels, index, value)\n\n\nThis function splits the labels into two subsets based on the provided feature and value. It handles both numerical and categorical features.\n\nArguments\n\ndata::AbstractMatrix{T}: A matrix of features, where each row is a data point and each column is a feature.\nlabels::AbstractVector: A vector of labels corresponding to the data points.\nindex::Int: The index of the feature to split on.\nvalue::T: The value to split the feature on.\n\nReturns\n\nTuple{AbstractVector, AbstractVector}: A tuple containing the left and right sets of labels.\n\n\n\n\n\n","category":"method"},{"location":"#DecisionTreeAndRandomForest.split_node_vr-Union{Tuple{T}, Tuple{AbstractMatrix{T}, AbstractVector, Int64, T}} where T","page":"Home","title":"DecisionTreeAndRandomForest.split_node_vr","text":"split_node_vr(data, labels, index, value)\n\n\nSplits the labels into two nodes based on the provided feature and value.\n\nArguments\n\ndata::AbstractMatrix{T}: A matrix of features, where each row is a data point and each column is a feature.\nlabels::AbstractVector: A vector of labels corresponding to the data points.\nindex::Int: The index of the feature to split on.\nvalue::T: The value to split the feature on.\n\nReturns\n\nTuple{AbstractVector, AbstractVector}: A tuple containing the left and right sets of labels.\n\n\n\n\n\n","category":"method"},{"location":"#DecisionTreeAndRandomForest.split_variance","page":"Home","title":"DecisionTreeAndRandomForest.split_variance","text":"This function is a wrapper for find_best_split_vr to be used as the split criterion in the build_tree function.\n\nArguments\n\ndata::AbstractMatrix: A matrix of features, where each row is a data point and each column is a feature.\nlabels::AbstractVector: A vector of labels corresponding to the data points.\nnum_features::Int: The number of features to consider for each split.\n\nReturns\n\nTuple{Int, T}: A tuple containing the index of the best feature and the best split value.\n\n\n\n\n\n","category":"function"},{"location":"#DecisionTreeAndRandomForest.variance-Tuple{AbstractVector}","page":"Home","title":"DecisionTreeAndRandomForest.variance","text":"variance(y)\n\n\nCalculate the sample variance of a given set of labels. It uses the standard formula for sample variance.\n\nArguments\n\ny::AbstractVector: A vector of numerical labels for which the variance is to be computed.\n\nReturns\n\nFloat64: The sample variance of the input label vector y.\n\n\n\n\n\n","category":"method"},{"location":"#DecisionTreeAndRandomForest.variance_reduction-Union{Tuple{T}, Tuple{T, T}} where T<:(AbstractVector)","page":"Home","title":"DecisionTreeAndRandomForest.variance_reduction","text":"variance_reduction(left_dataset, right_dataset)\n\n\nCalculates the variance reduction achieved by a split.\n\nArguments\n\nleft_dataset::AbstractVector{T}: A vector of labels for the left subset of the data.\nright_dataset::AbstractVector{T}: A vector of labels for the right subset of the data.\n\nReturns\n\nFloat64: The variance reduction achieved by the split.\n\n\n\n\n\n","category":"method"},{"location":"#DecisionTreeAndRandomForest.weighted_gini-Union{Tuple{T}, Tuple{T, T}} where T<:(AbstractVector)","page":"Home","title":"DecisionTreeAndRandomForest.weighted_gini","text":"weighted_gini(left_dataset, right_dataset)\n\n\nThis function calculates the weighted Gini impurity of a split, which is a measure of the impurity of the split considering the size of each subset. It's used to evaluate the quality of a split in a decision tree.\n\nArguments\n\nleft_dataset::AbstractVector{T}: A vector of labels for the left subset of the data.\nright_dataset::AbstractVector{T}: A vector of labels for the right subset of the data.\n\nReturns\n\nFloat64: The weighted Gini impurity of the split.\n\n\n\n\n\n","category":"method"}]
}
