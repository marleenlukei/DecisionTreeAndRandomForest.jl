<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Dataset Examples · DecisionTreeAndRandomForest.jl</title><meta name="title" content="Dataset Examples · DecisionTreeAndRandomForest.jl"/><meta property="og:title" content="Dataset Examples · DecisionTreeAndRandomForest.jl"/><meta property="twitter:title" content="Dataset Examples · DecisionTreeAndRandomForest.jl"/><meta name="description" content="Documentation for DecisionTreeAndRandomForest.jl."/><meta property="og:description" content="Documentation for DecisionTreeAndRandomForest.jl."/><meta property="twitter:description" content="Documentation for DecisionTreeAndRandomForest.jl."/><meta property="og:url" content="https://marleenlukei.github.io/DecisionTreeAndRandomForest.jl/dataset_examples/"/><meta property="twitter:url" content="https://marleenlukei.github.io/DecisionTreeAndRandomForest.jl/dataset_examples/"/><link rel="canonical" href="https://marleenlukei.github.io/DecisionTreeAndRandomForest.jl/dataset_examples/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">DecisionTreeAndRandomForest.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting_started/">Getting Started</a></li><li><a class="tocitem" href="../splitting_criterion/">Splitting Criterion</a></li><li class="is-active"><a class="tocitem" href>Dataset Examples</a><ul class="internal"><li><a class="tocitem" href="#Classification-Tree-and-Random-Forest-Classifying-Iris-Types"><span>Classification Tree and Random Forest - Classifying Iris Types</span></a></li><li><a class="tocitem" href="#Regression-Tree-and-Random-Forest-Predicting-Housing-Prices"><span>Regression Tree and Random Forest - Predicting Housing Prices</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Dataset Examples</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Dataset Examples</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/marleenlukei/DecisionTreeAndRandomForest.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/marleenlukei/DecisionTreeAndRandomForest.jl/blob/main/docs/src/dataset_examples.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Decision-Trees-and-Random-Forests-on-Real-World-Datasets"><a class="docs-heading-anchor" href="#Decision-Trees-and-Random-Forests-on-Real-World-Datasets">Decision Trees and Random Forests on Real-World Datasets</a><a id="Decision-Trees-and-Random-Forests-on-Real-World-Datasets-1"></a><a class="docs-heading-anchor-permalink" href="#Decision-Trees-and-Random-Forests-on-Real-World-Datasets" title="Permalink"></a></h1><h2 id="Classification-Tree-and-Random-Forest-Classifying-Iris-Types"><a class="docs-heading-anchor" href="#Classification-Tree-and-Random-Forest-Classifying-Iris-Types">Classification Tree and Random Forest - Classifying Iris Types</a><a id="Classification-Tree-and-Random-Forest-Classifying-Iris-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Classification-Tree-and-Random-Forest-Classifying-Iris-Types" title="Permalink"></a></h2><p>In this tutorial, we will demonstrate how to use the package to create a classification tree and a random forest, and apply them to classify the Iris dataset.</p><p>First, ensure to import all the necessary packages:</p><pre><code class="language-julia hljs">using DataFrames
using MLJ: load_iris, unpack, partition
using DecisionTreeAndRandomForest</code></pre><p>Second, we load the Iris dataset and prepare the data by splitting it into training and test sets:</p><pre><code class="language-julia hljs">data = load_iris()
iris = DataFrame(data)
y, X = unpack(iris, ==(:target); rng=123)
train, test = partition(eachindex(y), 0.7)
train_labels = Vector{String}(y[train])
test_labels = Vector{String}(y[test])
train_data = Matrix(X[train, :])
test_data = Matrix(X[test, :])</code></pre><h3 id="Decision-Tree"><a class="docs-heading-anchor" href="#Decision-Tree">Decision Tree</a><a id="Decision-Tree-1"></a><a class="docs-heading-anchor-permalink" href="#Decision-Tree" title="Permalink"></a></h3><p>Next, we create the decision tree and fit it to the training data:</p><pre><code class="language-julia hljs"># Initialize the classification tree with hyperparameters:
# - max_depth: Maximum depth of the tree (-1 means no limit).
# - min_samples_split: Minimum number of samples required to split an internal node.
# - num_features: Number of features to consider when looking for the best split (-1 means all features).
# - split_criterion: Function to measure the quality of a split.
tree = DecisionTree(-1, 1, -1, split_gini)
fit!(tree, train_data, train_labels)</code></pre><p>Now we can use the Decision Tree to make predictions for unseen data:</p><pre><code class="language-julia hljs">predictions = predict(tree, test_data)
println(&quot;Decision Tree - Correct label: &quot;, test_labels[1])
println(&quot;Decision Tree - Predicted label: &quot;, predictions[1])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Decision Tree - Correct label: versicolor
Decision Tree - Predicted label: versicolor</code></pre><p>We can also test the accuracy of our Classification Tree:</p><pre><code class="language-julia hljs">accuracy = sum(predictions .== test_labels) / length(test_labels)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">0.9555555555555556</code></pre><h3 id="Random-Forest"><a class="docs-heading-anchor" href="#Random-Forest">Random Forest</a><a id="Random-Forest-1"></a><a class="docs-heading-anchor-permalink" href="#Random-Forest" title="Permalink"></a></h3><p>Next, we create the random forest and fit it to the training data:</p><pre><code class="language-julia hljs"># Initialize the random forest with hyperparameters:
# - max_depth: Maximum depth of the trees (-1 means no limit).
# - min_samples_split: Minimum number of samples required to split an internal node.
# - split_criterion: Function to measure the quality of a split.
# - number_of_trees: Number of trees in the forest.
# - subsample_percentage: Percentage of samples to use for each tree (0.8 means 80% of the data).
# - num_features: Number of features to consider when looking for the best split (-1 means all features).
forest = RandomForest(7, 3, split_gini, 15, 0.7, -1)  # Using 7 for max_depth, 3 for min_samples_split, 15 for number_of_trees, and 0.7 for subsample_percentage
fit!(forest, train_data, train_labels)</code></pre><p>Now we can use the Random Forest to make predictions for unseen data:</p><pre><code class="language-julia hljs">forest_predictions = predict(forest, test_data)
println(&quot;Random Forest - Correct label: &quot;, test_labels[1])
println(&quot;Random Forest - Predicted label: &quot;, forest_predictions[1])</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Random Forest - Correct label: versicolor
Random Forest - Predicted label: versicolor</code></pre><p>We can also test the accuracy of our Random Forest:</p><pre><code class="language-julia hljs">forest_accuracy = sum(forest_predictions .== test_labels) / length(test_labels)</code></pre><h2 id="Regression-Tree-and-Random-Forest-Predicting-Housing-Prices"><a class="docs-heading-anchor" href="#Regression-Tree-and-Random-Forest-Predicting-Housing-Prices">Regression Tree and Random Forest - Predicting Housing Prices</a><a id="Regression-Tree-and-Random-Forest-Predicting-Housing-Prices-1"></a><a class="docs-heading-anchor-permalink" href="#Regression-Tree-and-Random-Forest-Predicting-Housing-Prices" title="Permalink"></a></h2><p>We will now demonstrate how to use the package to create a regression tree and a random forest, and apply them to the Boston Housing dataset. The dataset contains information collected by the U.S. Census Service concerning housing in the area of Boston, Massachusetts. It is often used to predict the median value of owner-occupied homes (in <span>$</span>1000s).</p><p>First, ensure to import all the necessary packages:</p><pre><code class="language-julia hljs">using MLJ: unpack, partition
using RDatasets: dataset
using DataFrames
using DecisionTreeAndRandomForest
using Statistics: mean</code></pre><p>Second, we load the Boston Housing dataset and prepare the data by splitting it into training and test sets:</p><pre><code class="language-julia hljs">boston = dataset(&quot;MASS&quot;, &quot;Boston&quot;)
data = DataFrame(boston)
X = data[:, 1:end-1]
y = data[:, end]
train_indices, test_indices = partition(eachindex(y), 0.95, rng=123)
train_labels = Vector{Float64}(y[train_indices])
test_labels = Vector{Float64}(y[test_indices])
train_data = Matrix(X[train_indices, :])
test_data = Matrix(X[test_indices, :])</code></pre><h3 id="Decision-Tree-2"><a class="docs-heading-anchor" href="#Decision-Tree-2">Decision Tree</a><a class="docs-heading-anchor-permalink" href="#Decision-Tree-2" title="Permalink"></a></h3><p>Next, we create the decsion tree and fit it to the training data:</p><pre><code class="language-julia hljs">tree = DecisionTree(-1, 1, -1, split_variance)
fit!(tree, train_data, train_labels)</code></pre><p>Now we can use the Regression Tree to make predictions for unseen data:</p><pre><code class="language-julia hljs">predictions = predict(tree, test_data)</code></pre><p>We can also assess the quality of our Regression Tree:</p><pre><code class="language-julia hljs">mse = mean((predictions .- test_labels).^2)
println(&quot;Decision Tree - Mean Squared Error: &quot;, mse)
ss_res = sum((test_labels .- predictions).^2)
ss_tot = sum((test_labels .- mean(test_labels)).^2)
r2_score = 1 - (ss_res / ss_tot)
println(&quot;Decision Tree - R² Score: &quot;, r2_score)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Decision Tree - Mean Squared Error: 11.843422222222225
Decision Tree - R² Score: 0.7048454798627652</code></pre><h3 id="Random-Forest-2"><a class="docs-heading-anchor" href="#Random-Forest-2">Random Forest</a><a class="docs-heading-anchor-permalink" href="#Random-Forest-2" title="Permalink"></a></h3><p>Next, we create the random forest and fit it to the training data:</p><pre><code class="language-julia hljs">forest = RandomForest(-1, 1, split_variance, 10, 0.8, -1)
fit!(forest, train_data, train_labels)</code></pre><p>Now we can use the Random Forest to make predictions for unseen data:</p><pre><code class="language-julia hljs">forest_predictions = predict(forest, test_data)</code></pre><p>We can also assess the quality of our Random Forest:</p><pre><code class="language-julia hljs">forest_mse = mean((forest_predictions .- test_labels).^2)
println(&quot;Random Forest - Mean Squared Error: &quot;, forest_mse)
forest_ss_res = sum((test_labels .- forest_predictions).^2)
forest_ss_tot = sum((test_labels .- mean(test_labels)).^2)
forest_r2_score = 1 - (forest_ss_res / forest_ss_tot)
println(&quot;Random Forest - R² Score: &quot;, forest_r2_score)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Random Forest - Mean Squared Error: 3.8126026336734706
Random Forest - R² Score: 0.9049846505763851</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../splitting_criterion/">« Splitting Criterion</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Saturday 13 July 2024 08:42">Saturday 13 July 2024</span>. Using Julia version 1.10.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
